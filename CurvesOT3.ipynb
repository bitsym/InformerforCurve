{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5p4wM87Veqw0JLDRezjML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitsym/InformerforCurve/blob/main/CurvesOT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oepnem9hkH7u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, MultiHeadAttention, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "def build_transformer(num_features, sequence_length, d_model, num_heads, ff_dim, num_layers, rate=0.1):\n",
        "    inputs = Input(shape=(sequence_length, num_features))\n",
        "    # Map the input features to the model dimension\n",
        "    x = Dense(d_model, activation='relu')(inputs)\n",
        "    # Add positional encodings\n",
        "    x = PositionalEncoding(sequence_length, d_model)(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoder(d_model, num_heads, ff_dim, rate)(x)\n",
        "    outputs = Dense(1, activation='linear')(x)  # Predicting 'OT1'\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('CurvesT3.csv')\n",
        "\n",
        "# Select features and target\n",
        "features = data[['LOCATION', 'X SERIAL', 'Y SERIAL', 'Z SERIAL']]\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lz5FVLInc-P",
        "outputId": "d289933f-307e-44fc-e3e3-70d45893dce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     LOCATION   X SERIAL   Y SERIAL   Z SERIAL      OT3 \n",
            "0    0.000000   0.000000   0.000000   0.000000 -0.242587\n",
            "1    0.050267  -2.422151  -1.368757   3.929735 -0.326288\n",
            "2    0.100688  -5.581641  -3.148854   7.076572 -0.425911\n",
            "3    0.150950  -9.442119  -5.338873   8.929961 -0.475210\n",
            "4    0.200955 -13.505708  -7.671132  10.064515 -0.488859\n",
            "..        ...        ...        ...        ...       ...\n",
            "331  0.801434 -64.007807 -36.688473  -3.398382 -0.450358\n",
            "332  0.851568 -67.827846 -38.824129  -5.649999 -0.434834\n",
            "333  0.901326 -71.574625 -40.908549  -8.068927 -0.439284\n",
            "334  0.950659 -75.440764 -43.085383 -10.197360 -0.464620\n",
            "335  1.000000 -79.496033 -45.445277 -11.682654 -0.482181\n",
            "\n",
            "[336 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = data[]"
      ],
      "metadata": {
        "id": "CHN1kSo8n12C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('CurvesT3.csv')\n",
        "\n",
        "# Select features and target\n",
        "features = data[['LOCATION', 'X SERIAL', 'Y SERIAL', 'Z SERIAL']]\n",
        "target = data['OT3']\n",
        "\n",
        "# Normalize the feature data\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Check the sequence creation logic\n",
        "def create_sequences(features, target, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(0, len(features) - sequence_length + 1, sequence_length):  # Ensure no overlap if not desired\n",
        "        X.append(features[i:(i + sequence_length)])\n",
        "        y.append(target[i + sequence_length - 1])  # Adjust based on how you want to predict\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Assuming sequence_length is correctly set and there's enough data\n",
        "features_seq, target_seq = create_sequences(features_scaled, target, sequence_length=21)\n",
        "print(\"Generated Features Shape:\", features_seq.shape)\n",
        "print(\"Generated Target Shape:\", target_seq.shape)\n",
        "\n",
        "# Generate sequences\n",
        "sequence_length = 21\n",
        "features_seq, target_seq = create_sequences(features_scaled, target, sequence_length)\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "# Note: Adjust indices based on how you define training, validation, and testing\n",
        "index_train = len(features_seq) - 2 * sequence_length  # Reserve the last 42 sequences\n",
        "index_val = len(features_seq) - sequence_length  # Reserve the last 21 sequences\n",
        "\n",
        "train_features = features_seq[:index_train]\n",
        "train_target = target_seq[:index_train]\n",
        "\n",
        "val_features = features_seq[index_train:index_val]\n",
        "val_target = target_seq[index_train:index_val]\n",
        "\n",
        "test_features = features_seq[index_val:]\n",
        "test_target = target_seq[index_val:]\n",
        "\n",
        "# Now you can proceed to define and train your model with train_features, val_features, and test_features\n",
        "# Assuming the total number of sequences and the sequence length\n",
        "total_sequences = features_seq.shape[0]\n",
        "train_features = features_seq[:int(total_sequences * 0.8)]\n",
        "train_target = target_seq[:int(total_sequences * 0.8)]\n",
        "val_features = features_seq[int(total_sequences * 0.8):int(total_sequences * 0.9)]\n",
        "val_target = target_seq[int(total_sequences * 0.8):int(total_sequences * 0.9)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "ePIzKwcsmC1-",
        "outputId": "a5ec90b1-6a63-48b1-c8f4-2acd161d9876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'OT3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'OT3'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c174bea5fc05>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Select features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LOCATION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X SERIAL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y SERIAL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Z SERIAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OT3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Normalize the feature data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'OT3'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Adjust model dimensions and architecture\n",
        "d_model = 1024  # increased from 512\n",
        "ff_dim = 4096   # increased from 2048\n",
        "num_layers = 4  # increased from 2\n",
        "\n",
        "model = build_transformer(num_features=4, sequence_length=21, d_model=d_model, num_heads=8, ff_dim=ff_dim, num_layers=num_layers)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Add learning rate scheduler and early stopping\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_features, train_target,\n",
        "    validation_data=(val_features, val_target),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "gJLPpkRnmEEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot learning rate vs loss\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Rate vs. Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I4s_jprjmFNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sequences = len(features_seq)\n",
        "print(\"Total sequences available:\", total_sequences)\n",
        "# Make predictions on the test dataset\n",
        "test_predictions = model.predict(test_features)\n",
        "test_pre = test_predictions[0,:]\n",
        "print(test_pre)\n",
        "testPre = test_pre[:,0]\n",
        "print(testPre)"
      ],
      "metadata": {
        "id": "ukHogx1gmGpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_target)\n",
        "print(target)\n",
        "testY = target[-21:]\n",
        "print(testY)\n",
        "print(testY)\n",
        "print(len(testY))\n",
        "testY = testY.to_numpy()\n"
      ],
      "metadata": {
        "id": "fBDv-bE8mIGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming test_target is correctly aligned with test_predictions\n",
        "# plt.figure(figsize=(12, 6))\n",
        "plt.plot(testY, label='Actual Values', linestyle='-', color='blue')\n",
        "plt.plot(testPre, label='Predicted Values', linestyle='--', color='red')\n",
        "plt.title('Comparison of Actual and Predicted Values')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "19RqPXg3mJOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with both testY and testPre\n",
        "data = {\n",
        "    'Actual Values': testY,\n",
        "    'Predicted Values': testPre\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "# Saving the DataFrame to a CSV file\n",
        "df.to_csv('predictions_comparison_OT3.csv', index=False)\n",
        "# Saving each vector to its own CSV file\n",
        "df_actual = pd.DataFrame(testY, columns=['Actual Values'])\n",
        "df_predicted = pd.DataFrame(testPre, columns=['Predicted Values'])\n",
        "\n",
        "df_actual.to_csv('actual_valuesOT3.csv', index=False)\n",
        "df_predicted.to_csv('predicted_valuesOT3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "KXL7xQarmKjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}